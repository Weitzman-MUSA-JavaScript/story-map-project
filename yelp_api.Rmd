---
title: "Data Collection: Yelp Fusion API"
output: html_document
theme: "flatly"
date: "September 9, 2024"
author: Anna Duan, annaduan@sas.upenn.edu
---
# Introduction  
This markdown gathers business listing data from Yelp Fusion API's Business Search Endpoint. We use OmaymaS's [`yelpr`](https://github.com/OmaymaS/yelpr) package to make the API call itself, and `httr` to parse through the response. We also use `tigris` to generate a list of all ZIP codes in Philadelphia to use as location specifications for the API call, then we later use these boundaries to filter out businesses that are not within the city boundaries.  
  
To use [Yelp Fusion API](https://docs.developer.yelp.com/docs/fusion-intro), first register for a Yelp account, then navigate to [manage API access](https://www.yelp.com/login?return_url=/developers/v3/manage_app) to find your API key.  
  
## Libraries  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      eval = FALSE)
# libraries 
library(devtools)
library(tigris)
library(tidyverse)
library(sf)
library(httr)
library(yelpr)
```

## Tigris boundaries 
Using `tigris`, we access ZIP code boundaries and names from the US Census Bureau. Using the resulting sf object, we create a list of Philadelphia ZIP codes and a unioned Philadelphia boundary object.
```{r load tigris boundaries}
neighs <- st_read("https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson") %>%
  erase_water() %>%
  st_transform(4326) %>%
  select(MAPNAME, Shape_Area)

neigh_coords <- neighs %>%
  st_centroid() %>% 
  mutate(lon = st_coordinates(.)[,1], lat = st_coordinates(.)[,2])

tracts <- tracts("PA", "Philadelphia") %>%
  st_transform(4326) %>%
  select(NAME)
```

# Yelp Fusion API  
## Helper function - `get_yelp()`  
To parse the results of the API call, a JSON file, we use a helper function to extract the fields we need. After creating a list of the specifications for the API call, we use `httr::GET()` to make the call. The result is a JSON file, which we flatten and and parse using `httr::content()`.   

We then retrieve specific columns, such as business name, latitude, longitude, alias, and rating as dataframes which we later concatenate into a larger dataframe.    

```{r get_yelp helper function}
#### Yelp api call function ####
url <- "https://api.yelp.com/v3/businesses/search"


# Define the get_yelp function with corrected error handling
get_yelp <- function(category, lat, lon, offset_num) {
  # args are category of business, latitude, longitude, and number to offset by
  
  queryString <- list(
    latitude = lat, 
    longitude = lon,
    term = category,
    sort_by = "distance",
    radius = 800,        # 1 mile in meters
    limit = 50,          # Maximum for Yelp Fusion API
    offset = offset_num  # Offset for pagination
  )
  
  # Use "GET" verb to request information from Yelp API
  response <- VERB(
    "GET",
    url,
    query = queryString,
    add_headers('Authorization' = 'Bearer u6vZZfFrAnI1IHH0p40WwmEB2W8yp8l-r1m8lmLmoYJCuq8obxrgyNYsgRJXwkqD1E4ceSmLuMEIuKUwF0yvv0Pb1GOO2TwtCaDfKl1DJ_L9xSElW4kCigyWG3tCZXYx'),
    content_type("application/octet-stream"),
    accept("application/json")
  )
  
  print("response received") 
  
  # Turn the response into a JSON object
  yelp.json <- httr::content(response, "parsed", flatten = TRUE, simplify = TRUE)
  
  print("yelp.json created") 
  
  # Check if businesses are returned
  if(is.null(yelp.json$businesses) || length(yelp.json$businesses) == 0) {
    message("No businesses found for this query.")
    return(data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    ))
  }
  
  # Retrieve columns from JSON structure
  biz.name <- data.frame(yelp.json$businesses$name, stringsAsFactors = FALSE)
  biz.rating <- data.frame(yelp.json$businesses$rating, stringsAsFactors = FALSE)
  biz.addr <- data.frame(yelp.json$businesses$location.address1, stringsAsFactors = FALSE)
  biz.lat <- data.frame(yelp.json$businesses$coordinates.latitude, stringsAsFactors = FALSE)
  biz.lon <- data.frame(yelp.json$businesses$coordinates.longitude, stringsAsFactors = FALSE)
  
  print("columns retrieved")
  
  # Bind the columns into one dataframe
  yelp.df <- cbind(biz.name, biz.rating, biz.addr, biz.lat, biz.lon) %>%
    as.data.frame()
  
  print(str(yelp.df))
  
  colnames(yelp.df) <- c("name", "rating", "address", "lat", "lon")
  
  print("columns renamed")
  
  # Add in category alias/title (this will give us cuisine information)
  cuisine <- yelp.json$businesses$categories
  
  cuis.df <- map_dfr(cuisine, function(x) {
    tibble(
      alias = paste(x$alias, collapse = ", "),
      title = paste(x$title, collapse = ", ")
    ) %>%
      as.data.frame()
  })
  
  print("cuisines retrieved")
  
  yelp.df <- cbind(yelp.df, cuis.df)
  
  print("cuisines added")
  
  # Ensure that the dataframe has the correct structure
  if(nrow(yelp.df) == 0) {
    yelp.df <- data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    )
  }
  
  return(yelp.df)
}

```

## Making the API call  
To make the API call, we write a nested dataframe to get around the 50 business limit. In order to capture all of the businesses in the city, we iterate through our list of ZIP codes, for which we iterate through offset values 0 to 10. This way, we can capture a maximum of 500 businesses per ZIP code, which is more than enough for most categories. For each loop, we store the output in a nested dataframe.      
```{r call api}
# Neighborhood names
neigh_list <- neigh_coords$MAPNAME

# Initialize a named list of empty dataframes for each offset
initialize_named_dfs <- function(neighs) { 
  empty_df <- data.frame(
    name = character(0), 
    rating = numeric(0),  
    address = character(0), 
    lat = numeric(0), 
    lon = numeric(0),
    alias = character(0),
    title = character(0),
    stringsAsFactors = FALSE
  )
  
  named_list <- setNames(
    lapply(neighs, function(neigh) empty_df),
    neighs
  )
  
  return(named_list)
}

# Number of offsets (e.g., 5 offsets for 0 to 200)
num_offsets <- 5

# Initialize the master list for all offsets
offset_list <- setNames(
  lapply(0:(num_offsets - 1), function(x) initialize_named_dfs(neigh_coords$MAPNAME)),
  paste0("offset_", 0:(num_offsets - 1))
)

# Function to safely fetch Yelp data with error handling
safe_get_yelp <- function(category, lat, lon, offset_num) {
  tryCatch({
    result <- get_yelp(category, lat, lon, offset_num)
    
    # Check if result has the expected columns
    expected_cols <- c("name", "rating", "address", "lat", "lon", "alias", "title")
    if(!all(expected_cols %in% colnames(result))) {
      stop("Returned dataframe has unexpected columns.")
    }
    
    return(result)
    
  }, error = function(e) {
    # Log the error message
    message(paste("Error fetching data for lat:", lat, "lon:", lon, "offset:", offset_num, "-", e$message))
    
    # Return an empty dataframe to maintain list structure
    return(data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    ))
  })
}

# Initialize active_neigh vector to track active neighborhoods
active_neigh <- setNames(rep(TRUE, length(neigh_list)), neigh_list)

# Loop through each offset (think of each offset as a page of results)
for (i in seq_along(offset_list)) {
  
  # Calculate the actual offset value (assuming each i corresponds to an offset increment of 50)
  actual_offset <- (i - 1) * 50
  
  # Loop through each neighborhood
  for (j in seq_len(nrow(neigh_coords))) {
    
    neigh <- neigh_coords$MAPNAME[j]
    lat <- neigh_coords$lat[j]
    lon <- neigh_coords$lon[j]
    
    # Check if the neighborhood is still active
    if (!active_neigh[neigh]) {
      next # Skip to the next neighborhood
    }
    
    print(paste("Batch", i, ", Neighborhood", j, ": ", neigh, ", Offset: ", actual_offset, sep = " "))
    
    # Fetch Yelp data safely using error handling
    fetched_data <- safe_get_yelp("restaurant", lat, lon, actual_offset)
    
    # Check if fetched_data has rows
    if (nrow(fetched_data) > 0) {
      offset_list[[i]][[as.character(neigh)]] <- fetched_data
    } else {
      message(paste("No data for:", neigh, "Offset:", actual_offset))
      # Mark the neighborhood as inactive
      active_neigh[neigh] <- FALSE
    }
    
  }
  
  Sys.sleep(1) # Pause for 1 second between each offset batch
}

```

# Data preparation    
## Store response as dataframe   
In this step, we take the nested dataframe from the last step and bind all of the dataframes inside to create one flattened dataframe. Using the longitude and latitude fields we got earlier, we transform it into a spatial features object with point locations for all of the businesses inside.   

```{r gather data to df}
# Combine all dataframes into one dataframe and remove duplicates
restaurants <- map_dfr(offset_list, ~ bind_rows(.x)) %>%
  unique() %>%
  filter(!is.na(lat)) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) %>%
  st_crop(st_union(neighs)) %>%
  filter(!is.na(title) | !is.na(alias)) %>%
   mutate(
    title = tolower(title),
    alias = tolower(alias),
    name = tolower(name)
  ) 

st_write(restaurants, "data/restaurants_raw.geojson", driver = "GeoJSON")
```

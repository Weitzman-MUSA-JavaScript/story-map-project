---
title: "Data Collection: Yelp Fusion API"
output: html_document
theme: "flatly"
date: "September 9, 2024"
author: Anna Duan, annaduan@sas.upenn.edu
---
# Introduction  
This markdown gathers business listing data from Yelp Fusion API's Business Search Endpoint. We use OmaymaS's [`yelpr`](https://github.com/OmaymaS/yelpr) package to make the API call itself, and `httr` to parse through the response. We also use `tigris` to generate a list of all ZIP codes in Philadelphia to use as location specifications for the API call, then we later use these boundaries to filter out businesses that are not within the city boundaries.  
  
To use [Yelp Fusion API](https://docs.developer.yelp.com/docs/fusion-intro), first register for a Yelp account, then navigate to [manage API access](https://www.yelp.com/login?return_url=/developers/v3/manage_app) to find your API key.  
  
## Libraries  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      eval = FALSE)
# libraries 
library(devtools)
library(tigris)
library(tidyverse)
library(sf)
library(httr)
library(mapview)
library(wordcloud2)
library(tm)
library(yelpr)
```

## Tigris boundaries 
Using `tigris`, we access ZIP code boundaries and names from the US Census Bureau. Using the resulting sf object, we create a list of Philadelphia ZIP codes and a unioned Philadelphia boundary object.
```{r load tigris boundaries}
neighs <- st_read("https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson") %>%
  erase_water() %>%
  st_transform(4326) %>%
  select(MAPNAME, Shape_Area)

neigh_coords <- neighs %>%
  st_centroid() %>% 
  mutate(lon = st_coordinates(.)[,1], lat = st_coordinates(.)[,2])

tracts <- tracts("PA", "Philadelphia") %>%
  st_transform(4326) %>%
  select(NAME)
```

# Yelp Fusion API  
## Helper function - `get_yelp()`  
To parse the results of the API call, a JSON file, we use a helper function to extract the fields we need. After creating a list of the specifications for the API call, we use `httr::GET()` to make the call. The result is a JSON file, which we flatten and and parse using `httr::content()`.   

We then retrieve specific columns, such as business name, latitude, longitude, alias, and rating as dataframes which we later concatenate into a larger dataframe.    

```{r get_yelp helper function}
#### Yelp api call function ####
url <- "https://api.yelp.com/v3/businesses/search"


# Define the get_yelp function with corrected error handling
get_yelp <- function(category, lat, lon, offset_num) {
  # args are category of business, latitude, longitude, and number to offset by
  
  queryString <- list(
    latitude = lat, 
    longitude = lon,
    term = category,
    sort_by = "distance",
    radius = 800,        # 1 mile in meters
    limit = 50,          # Maximum for Yelp Fusion API
    offset = offset_num  # Offset for pagination
  )
  
  # Use "GET" verb to request information from Yelp API
  response <- VERB(
    "GET",
    url,
    query = queryString,
    add_headers('Authorization' = 'Bearer u6vZZfFrAnI1IHH0p40WwmEB2W8yp8l-r1m8lmLmoYJCuq8obxrgyNYsgRJXwkqD1E4ceSmLuMEIuKUwF0yvv0Pb1GOO2TwtCaDfKl1DJ_L9xSElW4kCigyWG3tCZXYx'),
    content_type("application/octet-stream"),
    accept("application/json")
  )
  
  print("response received") 
  
  # Turn the response into a JSON object
  yelp.json <- httr::content(response, "parsed", flatten = TRUE, simplify = TRUE)
  
  print("yelp.json created") 
  
  # Check if businesses are returned
  if(is.null(yelp.json$businesses) || length(yelp.json$businesses) == 0) {
    message("No businesses found for this query.")
    return(data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    ))
  }
  
  # Retrieve columns from JSON structure
  biz.name <- data.frame(yelp.json$businesses$name, stringsAsFactors = FALSE)
  biz.rating <- data.frame(yelp.json$businesses$rating, stringsAsFactors = FALSE)
  biz.addr <- data.frame(yelp.json$businesses$location.address1, stringsAsFactors = FALSE)
  biz.lat <- data.frame(yelp.json$businesses$coordinates.latitude, stringsAsFactors = FALSE)
  biz.lon <- data.frame(yelp.json$businesses$coordinates.longitude, stringsAsFactors = FALSE)
  
  print("columns retrieved")
  
  # Bind the columns into one dataframe
  yelp.df <- cbind(biz.name, biz.rating, biz.addr, biz.lat, biz.lon) %>%
    as.data.frame()
  
  print(str(yelp.df))
  
  colnames(yelp.df) <- c("name", "rating", "address", "lat", "lon")
  
  print("columns renamed")
  
  # Add in category alias/title (this will give us cuisine information)
  cuisine <- yelp.json$businesses$categories
  
  cuis.df <- map_dfr(cuisine, function(x) {
    tibble(
      alias = paste(x$alias, collapse = ", "),
      title = paste(x$title, collapse = ", ")
    ) %>%
      as.data.frame()
  })
  
  print("cuisines retrieved")
  
  yelp.df <- cbind(yelp.df, cuis.df)
  
  print("cuisines added")
  
  # Ensure that the dataframe has the correct structure
  if(nrow(yelp.df) == 0) {
    yelp.df <- data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    )
  }
  
  return(yelp.df)
}

```

## Making the API call  
To make the API call, we write a nested dataframe to get around the 50 business limit. In order to capture all of the businesses in the city, we iterate through our list of ZIP codes, for which we iterate through offset values 0 to 10. This way, we can capture a maximum of 500 businesses per ZIP code, which is more than enough for most categories. For each loop, we store the output in a nested dataframe.      
```{r call api}
# 
# # Neighborhood names
# neigh_list <- neigh_coords$MAPNAME
# 
# # Initialize a named list of empty dataframes for each offset
# initialize_named_dfs <- function(zips) { 
#   empty_df <- data.frame(
#     name = character(0), 
#     rating = numeric(0),  
#     address = character(0), 
#     lat = numeric(0), 
#     lon = numeric(0),
#     alias = character(0),
#     title = character(0),
#     stringsAsFactors = FALSE
#   )
#   
#   named_list <- setNames(
#     lapply(zips, function(zip) empty_df),
#     zips
#   )
#   
#   return(named_list)
# }
# 
# # Number of offsets (e.g., 16 offsets for 0 to 750)
# num_offsets <- 5
# 
# # Initialize the master list for all offsets
# offset_list <- setNames(
#   lapply(0:(num_offsets - 1), function(x) initialize_named_dfs(neigh_coords$MAPNAME)),
#   paste0("offset_", 0:(num_offsets - 1))
# )
# 
# # Function to safely fetch Yelp data with error handling
# safe_get_yelp <- function(category, lat, lon, offset_num) {
#   tryCatch({
#     result <- get_yelp(category, lat, lon, offset_num)
#     
#     # Check if result has the expected columns
#     expected_cols <- c("name", "rating", "address", "lat", "lon", "alias", "title")
#     if(!all(expected_cols %in% colnames(result))) {
#       stop("Returned dataframe has unexpected columns.")
#     }
#     
#     return(result)
#     
#   }, error = function(e) {
#     # Log the error message
#     message(paste("Error fetching data for lat:", lat, "lon:", lon, "offset:", offset_num, "-", e$message))
#     
#     # Return an empty dataframe to maintain list structure
#     return(data.frame(
#       name = character(0), 
#       rating = numeric(0),  
#       address = character(0), 
#       lat = numeric(0), 
#       lon = numeric(0),
#       alias = character(0),
#       title = character(0),
#       stringsAsFactors = FALSE
#     ))
#   })
# }
# 
# # Loop through each offset (think of each offset as a page of results)
# for (i in seq_along(offset_list)) {
#   
#   # Calculate the actual offset value (assuming each i corresponds to an offset increment of 50)
#   actual_offset <- (i - 1) * 50
#   
#   # Loop through each ZIP code
#   for (j in seq_len(nrow(neigh_coords))) {
#     
#     neigh <- neigh_coords$MAPNAME[j]
#     lat <- neigh_coords$lat[j]
#     lon <- neigh_coords$lon[j]
#     
#     print(paste("Batch", i, ", Neighborhood", j, ": ", neigh, ", Offset: ", actual_offset, sep = " "))
#     
#     # Fetch Yelp data safely using error handling
#     fetched_data <- safe_get_yelp("restaurant", lat, lon, actual_offset)
#     
#     # Check if fetched_data has rows
#     if(nrow(fetched_data) > 0) {
#       offset_list[[i]][[as.character(neigh)]] <- fetched_data
#     } else {
#       message(paste("No data for:", neigh, "Offset:", actual_offset))
#     }
#     
#   }
#   
#   Sys.sleep(1) # Pause for 1 second between each offset batch
# }

# Neighborhood names
neigh_list <- neigh_coords$MAPNAME

# Initialize a named list of empty dataframes for each offset
initialize_named_dfs <- function(neighs) { 
  empty_df <- data.frame(
    name = character(0), 
    rating = numeric(0),  
    address = character(0), 
    lat = numeric(0), 
    lon = numeric(0),
    alias = character(0),
    title = character(0),
    stringsAsFactors = FALSE
  )
  
  named_list <- setNames(
    lapply(neighs, function(neigh) empty_df),
    neighs
  )
  
  return(named_list)
}

# Number of offsets (e.g., 5 offsets for 0 to 200)
num_offsets <- 5

# Initialize the master list for all offsets
offset_list <- setNames(
  lapply(0:(num_offsets - 1), function(x) initialize_named_dfs(neigh_coords$MAPNAME)),
  paste0("offset_", 0:(num_offsets - 1))
)

# Function to safely fetch Yelp data with error handling
safe_get_yelp <- function(category, lat, lon, offset_num) {
  tryCatch({
    result <- get_yelp(category, lat, lon, offset_num)
    
    # Check if result has the expected columns
    expected_cols <- c("name", "rating", "address", "lat", "lon", "alias", "title")
    if(!all(expected_cols %in% colnames(result))) {
      stop("Returned dataframe has unexpected columns.")
    }
    
    return(result)
    
  }, error = function(e) {
    # Log the error message
    message(paste("Error fetching data for lat:", lat, "lon:", lon, "offset:", offset_num, "-", e$message))
    
    # Return an empty dataframe to maintain list structure
    return(data.frame(
      name = character(0), 
      rating = numeric(0),  
      address = character(0), 
      lat = numeric(0), 
      lon = numeric(0),
      alias = character(0),
      title = character(0),
      stringsAsFactors = FALSE
    ))
  })
}

# Initialize active_neigh vector to track active neighborhoods
active_neigh <- setNames(rep(TRUE, length(neigh_list)), neigh_list)

# Loop through each offset (think of each offset as a page of results)
for (i in seq_along(offset_list)) {
  
  # Calculate the actual offset value (assuming each i corresponds to an offset increment of 50)
  actual_offset <- (i - 1) * 50
  
  # Loop through each neighborhood
  for (j in seq_len(nrow(neigh_coords))) {
    
    neigh <- neigh_coords$MAPNAME[j]
    lat <- neigh_coords$lat[j]
    lon <- neigh_coords$lon[j]
    
    # Check if the neighborhood is still active
    if (!active_neigh[neigh]) {
      next # Skip to the next neighborhood
    }
    
    print(paste("Batch", i, ", Neighborhood", j, ": ", neigh, ", Offset: ", actual_offset, sep = " "))
    
    # Fetch Yelp data safely using error handling
    fetched_data <- safe_get_yelp("restaurant", lat, lon, actual_offset)
    
    # Check if fetched_data has rows
    if (nrow(fetched_data) > 0) {
      offset_list[[i]][[as.character(neigh)]] <- fetched_data
    } else {
      message(paste("No data for:", neigh, "Offset:", actual_offset))
      # Mark the neighborhood as inactive
      active_neigh[neigh] <- FALSE
    }
    
  }
  
  Sys.sleep(1) # Pause for 1 second between each offset batch
}

```

# Data preparation    
## Store response as dataframe   
In this step, we take the nested dataframe from the last step and bind all of the dataframes inside to create one flattened dataframe. Using the longitude and latitude fields we got earlier, we transform it into a spatial features object with point locations for all of the businesses inside.   

```{r gather data to df}
# Combine all dataframes into one dataframe and remove duplicates
restaurants <- map_dfr(offset_list, ~ bind_rows(.x)) %>%
  unique() %>%
  filter(!is.na(lat)) %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) %>%
  st_crop(st_union(neighs)) %>%
  filter(!is.na(title) | !is.na(alias)) %>%
   mutate(
    title = tolower(title),
    alias = tolower(alias),
    name = tolower(name)
  ) 

st_write(restaurants, "data/restaurants.geojson", driver = "GeoJSON")
```

```{r wordcloud}
# Title
text <- restaurants$title
docs <- Corpus(VectorSource(text)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words) %>%
  filter(!word %in% c('food', 'fast', 'restaurants', 'delivery', 'services',
                      'trucks', 'grocery', 'shops', 'stores', 'sports', 'bars', 
                      'breakfast', 'brunch', 'new','convenience', 'stands', 'american'))

wordcloud2(data = df, size = 0.8, color = "random-light")
```
#### Cuisine Mapping #### 
```{r cuisine groups}
# Updated and Refined Cuisine Keywords Dictionary
cuisine_keywords <- list(
    American = c(
    "american", "tradamerican", "newamerican", "comfort", "comfortfood", "diners", 
    "pubs", "buffets", "steakhouses", "gastropubs", "bar", "bars", "sportsbars", "salad", "bakeries",
    "lounges", "brewpubs", "beergardens", "burger", "diner", "cheesesteaks", "mac and cheese",
    "luncheonette", "chicken_wings", "hotdogs", "wings", "cheesesteak", "steaks",
    "bbq", "barbecue", "barbeque", "smoked", "ribs", "pitmaster", "dog", "dogs", "grill",
    "fastfood", "quickbites", "burgers", "fries", "wraps", "pretzels", "grille", "coffee", "icecream",
    "deli", "sandwiches", "kosher", "delis", "cafes", "brunch", "hoagie", "brunch", "desserts",
    "dessert", "rotisserie", "sweetgreen", "crown", "fried chicken", "cafe", "chicken", "sports", "donuts", "vegan"
  ),
  
  Italian = c(
    "italian", "pasta", "lasagna", "trattoria", "pizzeria", "creperies", "pastashops", 
    "wine_bars", "tapas", "gelato", "risotto", "ristoranti", "ristorante", "pizza", "giuseppe",
    "trazza", "tuscan"
  ),
  
  United_Kingdom = c(
    "british", "english", "scottish", "irish", "welsh", "fish and chips", "fish & chips",
    "pub", "finnigan", "dandelion"),
  
  Chinese = c(
    "chinese", "shanghainese", "cantonese", "dimsum", "szechuan", "noodles", 
    "hotpot", "panasian", "wok", "china", "garden", "bubble tea", "boba", "dumpling",
    "wei", "meng", "dragon", "kung", "canton", "mandarin", "oriental", "zheng", "xi", "jiang", "dian",
    "ho", "lam", "yang", "zhong", "asian", "shing", "east", "pearl", "palace", "chen", "sai", "yong",
    "hou", "hong", "zhang", "jun", "golden", "kam", "dong", "sheng", "chung",
    "ping", "kon", "yi", "wah", "lee", "nanchang", "lim", "yoo", "chan", "choing", "chuong", "mui", "star", "lui", "yuan", "heng", "kee", "yue", "orient", "ming"
  ),
  
  Mexican = c(
    "mexican", "tacos", "texmex", "burritos", "enchiladas", "salsa", "margaritas", 
    "fajitas", "taqueria", "masa", "tex-mex"
  ),
  
  Japanese = c(
    "japanese", "sushi", "ramen", "teppanyaki", "izakaya", "sashimi", "hibachi", 
    "omakase", "shabu", "osaka"
  ),
  
  Korean = c(
    "korean", "bibimbap", "kimchi", "bulgogi", "kbbq", "koreanbbq", "seung", "kim"
  ),
  
  Middle_Eastern = c(
    "mideastern", "middle_eastern", "halal", "shawarma", "falafel", "kebab", 
    "hummus", "arabic", "syrian", "sahara", "afghan", "moroccan", "nile", "istanbul"
  ),
  
  Mediterranean = c(
    "mediterranean", "lebanese", "spanish", "iberian", "grille", "mezze", "baklava", "greek", "gyro", "souvlaki", "moussaka", "portuguese"
  ),
  
  Soul_Food = c(
    "soulfood", "southern", "soul", "cajun", "creole", "southern", "soul", "cajun", "creole",
    "cajuncreole"
  ),
    
  South_Asian = c(
    "indian", "indpak", "curry", "tandoori", "naan", "masala", "biriyani", "pakistani", "nepali", "himalayan"
  ),
  
  Thai = c(
    "thai", "pad_thai", "thaifusion"
  ),
  
  Vietnamese = c("pho", "viet", "vietnamese", "banh_mi", "pho", "bun", "pho", "springrolls", "trinh", "nguyen", "tran"),
  
  Ethiopian = c(
    "ethiopian", "injera", "doro_wot", "tibs"
  ),
  
  Caribbean = c(
    "caribbean", "jamaican", "trinidadian", "haitian", "puertorican", "soul", 
    "jerk", "dominican", "borinquen", "carribean"
  ),
  
  West_European = c(
    "french", "crepe", "bistro", "brasserie", "patisserie", "volksfest", "brahaus", "moderneuropean", "european", "german", "scandinavian", "brasseries"
  ),
  
  East_European = c(
    "russian", "uzbek", "ukrainian", "georgian", "ulfatlar", "khachapuri", "khinkali", "plov", "lagman", "polish"
  ),
  
  Latin_American = c(
    "latin", "latinamerican", "nicaraguan", "salvadoran", "brazilian", "colombian", "guatemalteco", "guadelupana", "casa", "guadalupana", "cancun", "caldos", "restaurante", "provocan", "de jesus", "honduran", "latinos", "venezuelan"

  ),
  
  Hawaiian = c(
    "hawaiian", "poke", "loco_moco", "laulau", "kalua_bbq"
  ),
  
  West_African = c(
    "african", "senegalese", "nigerian", "ghanaian", "cameroonian", "ivorian", "jollof"
  ),
  
  Southeast_Asian = c(
    "cambodian", "amok", "lok_lak", "malaysian", "indonesian", "kerala", "filipino"
  ),
  
  Seafood = c(
    "seafood", "crab", "lobster", "oyster", "clam", "shrimp", "fish", "scallops", "mussels", "crab", "lobster", "oyster", "clam", "shrimp", "fish", "scallops", "mussels", "sea food", "surf"
  )
)

# Define cuisine priority (from highest to lowest)
cuisine_priority <- c(
  "United_Kingdom", "South_Asian", "Thai", "Hawaiian", "West_African", 
  "Southeast_Asian", "Mexican", "Japanese", "Korean", "Vietnamese", "Ethiopian", 
  "Soul_Food", "Caribbean", "West_European", "East_European", 
  "Chinese", "Middle_Eastern", "Mediterranean",
  "Latin_American", "Italian", "Seafood", "American"
)

```

```{r cuisine mapping}
# Function to assign primary cuisine based on keywords with prioritization
assign_primary_cuisine <- function(name, title, alias, cuisine_dict, priority_order) {
  # Replace NA with empty string to avoid issues
  name <- ifelse(is.na(name), "", name)
  title <- ifelse(is.na(title), "", title)
  alias <- ifelse(is.na(alias), "", alias)
  
  # Combine name, title, alias for comprehensive search
  combined_text <- paste(name, title, alias, sep = " ")
  
  # Convert to lowercase for case-insensitive matching
  combined_text <- tolower(combined_text)
  
  # Iterate over each cuisine based on priority
  for (cuisine in priority_order) {
    keywords <- cuisine_dict[[cuisine]]
    
    # Create a regex pattern to match any of the keywords
    # Use word boundaries to match whole words
    # Escape any special regex characters in keywords
    escaped_keywords <- str_replace_all(keywords, "([.|()\\^{}+$*?]|\\[|\\]|\\\\)", "\\\\\\1")
    pattern <- paste0("\\b(", paste(escaped_keywords, collapse = "|"), ")\\b")
    
    # Check if any keyword matches
    if (grepl(pattern, combined_text, ignore.case = TRUE)) {
      return(cuisine)
    }
  }
  
  # If no match found, return NA
  return("Unknown")
}

restaurants$cuisine <- mapply(
  assign_primary_cuisine,
  name = restaurants$name,
  title = restaurants$title,
  alias = restaurants$alias,
  MoreArgs = list(cuisine_dict = cuisine_keywords, priority_order = cuisine_priority)
)

# View the first few entries with the assigned cuisine
restaurants %>%
  st_drop_geometry() %>%
  group_by(cuisine) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)

unclassified <- restaurants %>%
  st_drop_geometry() %>%
  filter(cuisine == "Unknown") %>%
  select(name, title, alias)

# Cuisine word cloud
text <- restaurants$cuisine
docs <- Corpus(VectorSource(text)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words) %>%
  filter(word != "american")


set.seed(1234) # for reproducibility 
wordcloud2(data = df, size = 0.8, color = "random-light")
```

#### Exploratory Analysis #### 
```{r cuisine distribution}

# Location of all restaurants
ggplot() + 
  geom_sf(data = restaurants, color = "darkcyan", size = 0.5) +
  labs(
    title = "Location of Restaurants in Philadelphia",
  ) +
  theme_void()

# Plot the distribution of cuisine types, ordered by count
exclude <- c("Unknown", "American", "Seafood")
ggplot() +
  geom_bar(data = restaurants %>% filter(!cuisine %in% exclude), aes(x = cuisine), show.legend = FALSE, fill = "darkcyan") +
  coord_flip() +
  scale_x_discrete(limits = restaurants %>% filter(!cuisine %in% exclude) %>% count(cuisine) %>% arrange(n) %>% pull(cuisine)) +
  labs(
    title = "Distribution of Cuisine Types",
    x = "Cuisine",
    y = "Restaurants"
  ) +
  theme_minimal() 
```

```{r map all restaurants}

cuisine_neigh <- st_intersection(restaurants, neighs) %>%
  st_drop_geometry() %>%
  left_join(neighs, by = "MAPNAME") %>%
  group_by(cuisine, MAPNAME) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(MAPNAME) %>%
  mutate(pct = 100 * count / sum(count)) %>%
  ungroup() %>%
  left_join(neighs, by = "MAPNAME") %>%
  st_as_sf()

cuisine_tract <- st_intersection(restaurants, tracts) %>%
  st_drop_geometry() %>%
  left_join(tracts, by = "NAME") %>%
  group_by(cuisine, NAME) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(NAME) %>%
  mutate(pct = 100 * count / sum(count)) %>%
  ungroup() %>%
  left_join(tracts, by = "NAME") %>%
  st_as_sf()

top_ethnic_cuisine_neigh <- cuisine_neigh %>% 
  filter(! cuisine %in% exclude) %>%
  group_by(MAPNAME) %>% 
  slice_max(order_by = pct)

top_ethnic_cuisine_tract <- cuisine_tract %>% 
  filter(! cuisine %in% exclude) %>%
  group_by(NAME) %>% 
  slice_max(order_by = pct)

# Map most common cuisine in each neighborhood
ggplot() +
  geom_sf(data = st_union(neighs), fill = "gray95", color = "transparent") +
  geom_sf(data = top_ethnic_cuisine_neigh %>% filter(count > 10), aes(fill = cuisine), color = "white") +
  scale_fill_brewer(palette = "Set3") +
  labs(
    title = "Most Common Cuisine in Each Neighborhood",
    fill = "Cuisine"
  ) +
  theme_void()

# Map most common cuisine in each census tract
ggplot() +
  geom_sf(data = st_union(neighs), fill = "gray95", color = "transparent") +
  geom_sf(data = top_ethnic_cuisine_tract %>% filter(count > 3), aes(fill = cuisine), color = "white") +
  scale_fill_brewer(palette = "Set3", direction = 1) +
  labs(
    title = "Census tracts with more than 30% of an ethnic cuisine",
    fill = "Cuisine"
  ) +
  theme_void()

```

```{r spruce hill}
spruce_hill_restaurants <- neighs %>%
  filter(MAPNAME %in% c("Spruce Hill")) %>%
  st_intersection(restaurants) 

# Plot and color by cuisine
ggplot() +
  geom_sf(data = spruce_hill_restaurants, aes(color = cuisine)) +
  scale_color_brewer(palette = "Set3") +
  labs(
    title = "Restaurants in Spruce Hill",
    fill = "Cuisine"
  ) +
  theme_void()

```
